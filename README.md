# Projet Big Data & Business Intelligence - DigiCheese

## ğŸ“Œ Description
Ce projet a pour objectif de mettre en oeuvre un ensemble de technologies **Big Data** (Hadoop, HBase, Python, HappyBase) et **Business intelligence** (Power BI) afin d'analyser un entrepÃ´t de donnÃ©es d'une fromagerie de merde.

Ã€ partir du fichier source `dataw_fro.csv` (donnÃ©es de commande depuis 2004), le projet est dÃ©coupÃ© en plusieurs **lots** :

- **LOT 0** : Nettoyage et prÃ©paration des donnÃ©es (Data Analyst)
- **Lot 1** : Traitement MapReduce Hadoop pour obtenir les 100 meilleurs commandes (2006-2010 dÃ©partements 53/61/28)
- **LOT 2** : Analyse complÃ©mentaire (2011-2016, dÃ©partements 22/49/53, Ã©chantillon alÃ©atoire 5%) avec export et graphiques
- **LOT 3** : Stockage et interrogation des donnÃ©es dans **Hbase** (NoSQL) via **HappyBase (Pyhon)**
- **LOT 4** : CrÃ©ation de **dashboards interactifs Power BI** connectÃ©s aux donnÃ©es

Le livrable final comprend : 

- Un **ensemble d'applications/scripts** (Hadoop, HBase, Python)
- Des **exports de rÃ©sultats** (CSV, Excel, PDF)
- Un **dashboard Power BI**
- Un **dossier documentaire** (analyses, procÃ©dures, recommandations)

## ğŸ“‚ Arborescence du dÃ©pÃ´t

```
/data/            â†’ DonnÃ©es sources et rÃ©sultats
   â”œâ”€â”€ dataw_fro.csv         (fichier source brut)
   â”œâ”€â”€ dataw_fro_clean.csv   (donnÃ©es nettoyÃ©es, LOT 0)
   â””â”€â”€ exports/              (rÃ©sultats en CSV, Excel, PDF)

/notebooks/       â†’ Notebooks Jupyter pour exploration et prototypes
   â””â”€â”€ 00_cleaning.ipynb     (nettoyage et analyse exploratoire)

/etl/             â†’ Scripts Hadoop Streaming (ETL)
   â”œâ”€â”€ mapper_lot1.py
   â”œâ”€â”€ reducer_lot1.py
   â”œâ”€â”€ mapper_lot2.py
   â””â”€â”€ reducer_lot2.py

/hbase/           â†’ Scripts et dÃ©finitions HBase
   â”œâ”€â”€ create_table.hbase    (DDL HBase)
   â”œâ”€â”€ import_csv.py         (import depuis CSV)
   â””â”€â”€ queries.py            (requÃªtes HappyBase)

/analytics_py/    â†’ Scripts analytiques Python
   â”œâ”€â”€ cleaning.py           (LOT 0 automatisÃ©)
   â”œâ”€â”€ lot3_exports.py       (exports CSV/XLSX/PDF depuis HBase)
   â””â”€â”€ tests/                (tests unitaires)

/powerbi/         â†’ Fichiers Power BI
   â”œâ”€â”€ projet_bigdata.pbix   (dashboard interactif)
   â””â”€â”€ sources.md            (notes de connexion Ã  HBase/CSV)

/docs/            â†’ Documentation projet
   â”œâ”€â”€ dictionnaire.md       (dictionnaire de donnÃ©es)
   â”œâ”€â”€ procedures_nettoyage.md
   â”œâ”€â”€ lot1.md, lot2.md, lot3.md (notes techniques par lot)
   â”œâ”€â”€ recommandations.md
   â””â”€â”€ rapport_final.pdf
```

## âš™ï¸ Installation & Environnement 

### PrÃ©requis

- **Python** â‰¥ 3.11
- **Java JDK** â‰¥ 8
- **Hadoop** (pseudo-distributed mode)
- **HBase** (avec ZooKeeper local)
- **Power BI Desktop** (Windows)

### DÃ©pendances Python

Les dÃ©pendances principales sont listÃ©es dans `requirements.txt` :

- `pandas`
- `numpy`
- `matplotlib`
- `openpyxl / xslxwriter`
- `happybase`
- `fastparquet / pyarrow`

Installation :

```bash
pip install -r requirements.txt
```

## ğŸš€ Utilisation

### - LOT 0 (Nettoyage)

```bash
python analytics_py/cleaning.py data/dataw_fro.csv data/dataw_fro_clean.csv
```


### - LOT 1 & 2 (Hadoop Streaming)

```bash
hdfs dfs -put data/dataw_fro_clean.csv /input/
hadoop jar hadoop-streaming.jar \
  -input /input/dataw_fro_clean.csv \
  -output /output/lot1 \
  -mapper etl/mapper_lot1.py \
  -reducer etl/reducer_lot1.py
```

### - LOT 3 (HBASE + HappyBase)

```bash
python hbase/import_csv.py data/dataw_fro_clean.csv
python analytics_py/lot3_exports.py
```

###- LOT 4 (POWER BI)

Ouvrir `powerbi/projet_bigdata.pbix` dans Power BI Desktop.

## ğŸ§¾ Documentation

Toutes les procÃ©dures dÃ©taillÃ©s (nettoyage, import HDFS, crÃ©ation HBase, requÃªtes Python, connexion Power BI) sont dÃ©crites dans le dossier `/docs/`.
